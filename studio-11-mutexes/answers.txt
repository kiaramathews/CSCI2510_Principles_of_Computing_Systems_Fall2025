Name: Kiara Mathews

1.  First, make a copy of your code from the previous studio where we
    created a race condition. Our goal today is to fix that race
    condition. We will do this by creating a `pthread_mutex_t` object,
    initializing it, and then locking and unlocking this mutex around
    the critical section.

    Create your `pthread_mutex_t` object with the help of the man page
    at `man pthread_mutex_init`. You may intialize your object with the
    static initializer (`PTHREAD_MUTEX_INITIALIZER`) or with the
    `pthread_mutex_init()` function (if you use this second you can
    leave the `pthread_mutexattr_t` pointer NULL). Create this mutex
    inside your `main()` function. Copy and paste your mutex creation
    code.

    pthread_mutex_t race_mutex;
    pthread_mutex_init(&race_mutex, NULL);


2.  Now you must pass a mutex pointer from your `main()` function to the
    `adder()` and `subtractor()` functions from last time. Pass these
    pointers through the fourth parameter of `pthread_create()`. Leave
    this answer blank.

3.  Take a moment to consider our goal. Previously, the simultaneous
    accesses to your variable caused some data corruption. What do you
    think the result will be if you synchronize your accesses? What will
    be the numeric value? How will this effect the amount of time your
    program takes to run?

    I think that if both the increments and decrements will not interfere with each other
    anymore if everthing is synchronized with a mutex. Therefore, the race would always 
    end in 0. However, the program will take longer to run because instead of running at the same time 
    interfering with eachother, the increments and decrements take turns.

4.  Now, use the functions `pthread_mutex_lock()` and
    `pthread_mutex_unlock()` to synchronize access to the racy variable.
    This means that your threads should not modify this variable unless
    they have successfully locked the mutex, and after modifying the
    variable they should unlock the mutex.

    Run your program many times. Does your program output match your
    expectations? Copy and paste several program outputs.

        Yes, it does my expectations.
        kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % gcc -pthread -o race_mutex race_mutex.c
    kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % ./race_mutex
    Final race value (with mutex): 0
    kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % ./race_mutex
    Final race value (with mutex): 0
    kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % ./race_mutex
    Final race value (with mutex): 0
    kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % ./race_mutex
    Final race value (with mutex): 0
    kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % ./race_mutex
    Final race value (with mutex): 0
    kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % 

5.  Now we want to quantify the overhead of using this mutex. Locking
    and unlocking the mutex is not free, but it is the cost we pay for
    correct behavior. Go back to your original racy program from the
    previous studio. Modify the number of iterations each thread
    performs to be twenty million (20,000,000). Take three timings of
    this program with the `time` utility. Copy and paste your program
    output, and calculate the average time.
    
kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race
Final race value: 266572
./race  0.02s user 0.00s system 4% cpu 0.409 total
kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race
Final race value: 450721
./race  0.01s user 0.00s system 116% cpu 0.013 total
kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race
Final race value: 463406
./race  0.01s user 0.00s system 117% cpu 0.014 total

AVERAGE TIME: 0.01333S

6.  Go back into your corrected program that uses your mutex. Set the
    number of iterations to be twenty million (20,000,000). You might
    have noticed that there are two possibilites for where you put your
    locking and unlocking code. You can have each thread perform one
    lock and unlock operation in total, or you can have each thread
    perform one lock and unlock operation for each individual increment
    or decrement of the racy variable.

    Modify your program, if necessary, so that each thread locks and
    unlocks the mutex once per increment or decrement (that is, each
    thread should lock and unlock the mutex twenty million times. Take
    three time measurements, copy and paste your output, and compute the
    average. How much longer did this execution take?

kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race_mutex
Final race value (with mutex): 0
./race_mutex  0.84s user 0.44s system 110% cpu 1.152 total
kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race_mutex
Final race value (with mutex): 0
./race_mutex  0.62s user 0.40s system 160% cpu 0.634 total
kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race_mutex
Final race value (with mutex): 0
./race_mutex  0.54s user 0.39s system 155% cpu 0.596 total

AVERAGE: 0.6667s

The execution took around 0.65s longer than the original race file.

7.  Modify your program so that you lock and unlock the mutex just once
    per thread. Repeat your timing experiment and copy/paste the output.
    How does this time compare to the original, racy program?

kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % gcc -pthread -o race_mutex race_mutex.c
kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race_mutex                      
Final race value: 0
./race_mutex  0.06s user 0.00s system 20% cpu 0.325 total
kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race_mutex
Final race value: 0
./race_mutex  0.06s user 0.00s system 97% cpu 0.065 total
kiaramathews@Kiaras-MacBook-Pro-2 studio-11-mutexes % time ./race_mutex
Final race value: 0
./race_mutex  0.06s user 0.00s system 97% cpu 0.065 total

AVERAGE: 0.6s

This execution is a lot more similar to the original race code time wise. 
However, the value is still remaining 0 like the new race code.

8.  Speculate about what might cause the results from the previous
    exercise.

    The per-thread locking version is much faster compared to the version that has to take the time to 
    lock and unlock on every single increment because it only performs one lock and unlock
    per thread, 2 operations total. 

10. Think of an example of where the first, per-iteration locking
    strategy might be more appropriate. Where might the second,
    per-thread locking strategy be more appropriate?

    Per-iteration locking is better when there are multiple threads updating the same thing
    and you want all of the updates to be saved saftely. Per-thread locking is better when one thread
    needs to do a lot of updates at one time without interuptions and the other threads can wait
    until all the work is finished.